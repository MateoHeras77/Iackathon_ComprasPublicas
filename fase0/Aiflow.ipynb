{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.providers.google.cloud.transfers.http_to_gcs import HttpToGcsOperator\n",
    "from airflow.providers.google.cloud.transfers.gcs_to_local import GoogleCloudStorageToLocaFileSystemOperator\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Configura las credenciales de Google Cloud Platform en Airflow antes de ejecutar este DAG\n",
    "\n",
    "def check_for_new_data(year):\n",
    "    url = f'https://datosabiertos.compraspublicas.gob.ec/PLATAFORMA/api/search_ocds?page=1&year={year}'\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Verificar si la solicitud HTTP es exitosa\n",
    "        data = response.json()\n",
    "        total_pages = data['pages']\n",
    "        \n",
    "        # Puedes comparar total_pages con el valor almacenado previamente para el año\n",
    "        # Si hay un cambio, significa que hay nuevas páginas de datos\n",
    "        # Actualiza el valor almacenado si es necesario\n",
    "        logging.info(f'Year: {year}, Total Pages: {total_pages}')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f'Error en la solicitud HTTP: {str(e)}')\n",
    "\n",
    "def download_data(year, task_instance, batch_size=1000):\n",
    "    bucket_name = 'airflow_iackathon'\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    url_base = 'https://datosabiertos.compraspublicas.gob.ec/PLATAFORMA/api/search_ocds?page={page}&year={year}'\n",
    "    try:\n",
    "        response = requests.get(url_base.format(page=1, year=year))\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        total_pages = data['pages']\n",
    "        \n",
    "        current_batch = []  # Almacena datos de la página actual\n",
    "        for page in range(1, total_pages + 1):\n",
    "            page_url = url_base.format(page=page, year=year)\n",
    "            page_response = requests.get(page_url)\n",
    "            page_response.raise_for_status()\n",
    "            page_data = page_response.json()\n",
    "            current_batch.extend(page_data['data'])\n",
    "            \n",
    "            if len(current_batch) >= batch_size or page == total_pages:\n",
    "                # Sube el archivo JSON de la página actual al final del lote o si es la última página\n",
    "                object_name = f'data/{year}_pages_{page - len(current_batch) + 1}_to_{page}.json'\n",
    "                blob = bucket.blob(object_name)\n",
    "                blob.upload_from_string(json.dumps(current_batch), content_type='application/json')\n",
    "                logging.info(f'Uploaded batch {page - len(current_batch) + 1} to {page} to GCS: {object_name}')\n",
    "                current_batch = []  # Reinicia el lote actual\n",
    "        \n",
    "        logging.info(f'Downloaded and uploaded data in batches for year {year}')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f'Error en la solicitud HTTP: {str(e)}')\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "    'data_download_dag',\n",
    "    schedule_interval='@daily',  # Configura el horario de ejecución según tus necesidades\n",
    "    start_date=datetime(2023, 1, 1),  # Fecha de inicio\n",
    "    catchup=False,  # Evita ponerse al día en la ejecución\n",
    ")\n",
    "\n",
    "check_for_new_data_task = PythonOperator(\n",
    "    task_id='check_for_new_data',\n",
    "    python_callable=check_for_new_data,\n",
    "    op_args=['{{ execution_date.year }}'],  # Año actual\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "download_data_task = PythonOperator(\n",
    "    task_id='download_data',\n",
    "    python_callable=download_data,\n",
    "    op_args=['{{ execution_date.year }}'],\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "upload_to_gcs_task = HttpToGcsOperator(\n",
    "    task_id='upload_to_gcs',\n",
    "    endpoint='/upload',  # Endpoint de GCS para subir el archivo\n",
    "    bucket_name='your-gcs-bucket-name',\n",
    "    object_name=\"{{ task_instance.xcom_pull(task_ids='download_data', key='gcs_object_name') }}\",\n",
    "    mime_type='application/json',\n",
    "    google_cloud_storage_conn_id='google_cloud_default',  # Debes configurar la conexión a GCS en Airflow\n",
    "    download_path=\"{{ task_instance.xcom_pull(task_ids='download_data', key='local_file_path') }}\",\n",
    "    task_id='upload_to_gcs',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "check_for_new_data_task >> download_data_task >> upload_to_gcs_task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
