{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parte 1/3: Subida de datos a la nube, la cual nos permitira alamcenar la BigData extraida de la API de Compras Públicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROYECTO PARA IACKATHON DE COMPRAS PUBLICAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 1:\n",
    "Este código es una solución efectiva y altamente escalable para la recopilación y almacenamiento de datos públicos de OCIDs (Identificadores de Contratos de Obra) de compras públicas en Ecuador. Su robustez radica en varios aspectos clave. \n",
    "- Primero, automatiza la recopilación de datos de manera programática, eliminando la necesidad de realizar esta tarea manualmente, lo que ahorra tiempo y recursos significativos. \n",
    "- Segundo, se integra con Google Cloud Storage, lo que garantiza un almacenamiento seguro y confiable en la nube, facilitando el acceso y la gestión de datos a largo plazo. Además, su diseño modular permite una escalabilidad sencilla: se pueden agregar más años de datos simplemente configurando la variable year_to_retrieve. \n",
    "\n",
    "Además, es altamente personalizable para la ejecución programada, ya sea mensual o quimestralmente, mediante la configuración de tareas cron o programación de trabajos periódicos, lo que asegura la actualización continua de los datos y su disponibilidad para análisis y toma de decisiones en curso. En resumen, este código ofrece una solución versátil y automatizada para la gestión de datos públicos en el contexto de compras públicas en Ecuador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import requests  # Importa la biblioteca para realizar solicitudes HTTP\n",
    "import pandas as pd  # Importa la biblioteca pandas para trabajar con datos tabulares\n",
    "from google.cloud import storage  # Importa la biblioteca de Google Cloud Storage para almacenamiento en la nube\n",
    "\n",
    "# Definir una función para obtener OCIDs de un año específico y una página específica\n",
    "def get_ocids(year, page):\n",
    "    # Construir la URL de búsqueda de OCIDs con el año y la página especificados\n",
    "    ocid_search_url = f\"https://datosabiertos.compraspublicas.gob.ec/PLATAFORMA/api/search_ocds?page={page}&year={year}\"\n",
    "    \n",
    "    # Realizar una solicitud HTTP GET a la URL\n",
    "    response = requests.get(ocid_search_url)\n",
    "    \n",
    "    # Verificar si la respuesta fue exitosa (código de estado 200)\n",
    "    if response.status_code == 200:\n",
    "        # Convertir la respuesta JSON en un diccionario Python\n",
    "        data = response.json()\n",
    "        \n",
    "        # Devolver la lista de OCIDs desde el diccionario de datos\n",
    "        return data.get('data', [])\n",
    "    else:\n",
    "        # Si la solicitud no fue exitosa, lanzar una excepción con un mensaje de error\n",
    "        raise Exception(f\"La solicitud para obtener OCIDs no fue exitosa para el año {year} y la página {page}.\")\n",
    "\n",
    "# Nombre del bucket de Google Cloud Storage donde se guardarán los datos\n",
    "bucket_name = os.getenv('bucket_name')\n",
    "\n",
    "# Año que deseas recuperar\n",
    "year_to_retrieve = 2022\n",
    "\n",
    "# Número de páginas para saltar en cada iteración\n",
    "page_jump = 100\n",
    "\n",
    "# Obtener el número total de páginas para el año seleccionado\n",
    "ocid_search_url = f\"https://datosabiertos.compraspublicas.gob.ec/PLATAFORMA/api/search_ocds?page=1&year={year_to_retrieve}\"\n",
    "response = requests.get(ocid_search_url)\n",
    "\n",
    "# Verificar si la respuesta fue exitosa\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    total_pages = data.get('pages', 0)\n",
    "else:\n",
    "    # Si la solicitud no fue exitosa, lanzar una excepción con un mensaje de error\n",
    "    raise Exception(f\"No se pudo obtener información para el año {year_to_retrieve}.\")\n",
    "\n",
    "# Iterar a través de las páginas para obtener los OCIDs y almacenarlos en GCS\n",
    "page = 1\n",
    "while page <= total_pages:\n",
    "    # Obtener la lista de OCIDs para la página actual\n",
    "    ocids = get_ocids(year_to_retrieve, page)\n",
    "    \n",
    "    if ocids:\n",
    "        # Convertir los datos en un DataFrame de pandas\n",
    "        df_ocids = pd.DataFrame(ocids)\n",
    "        \n",
    "        # Definir un nombre de archivo único para cada página\n",
    "        file_name = f\"datos/{year_to_retrieve}/ocids_{year_to_retrieve}_page_{page}.csv\"\n",
    "        \n",
    "        # Configurar la conexión a Google Cloud Storage\n",
    "        client = storage.Client()\n",
    "        \n",
    "        # Obtener el bucket de almacenamiento\n",
    "        bucket = client.bucket(bucket_name)\n",
    "        \n",
    "        # Crear un objeto blob en el bucket con el nombre de archivo\n",
    "        blob = bucket.blob(file_name)\n",
    "        \n",
    "        # Cargar el DataFrame como un archivo CSV en Google Cloud Storage\n",
    "        blob.upload_from_string(df_ocids.to_csv(index=False), content_type='text/csv')\n",
    "        \n",
    "        # Imprimir un mensaje indicando que los OCIDs se han almacenado en GCS\n",
    "        print(f\"Los OCIDs para la página {page} del año {year_to_retrieve} se han almacenado en GCS: {file_name}\")\n",
    "    \n",
    "    # Avanzar al siguiente conjunto de páginas\n",
    "    page += page_jump\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen del este primer paso:    \n",
    "- Importa las bibliotecas necesarias para realizar solicitudes HTTP, trabajar con datos tabulares y utilizar Google Cloud Storage.\n",
    "- Define una función llamada get_ocids(year, page) que realiza una solicitud HTTP GET para obtener OCIDs de un año específico y una página específica desde una URL proporcionada.\n",
    "- Especifica el nombre del bucket de Google Cloud Storage donde se guardarán los datos y el año que se desea recuperar.\n",
    "- Obtiene el número total de páginas disponibles para el año seleccionado a través de una solicitud HTTP.\n",
    "- Itera a través de las páginas, obtiene los OCIDs, los convierte en un DataFrame de pandas y los carga en Google Cloud Storage en archivos CSV. Luego, imprime un mensaje indicando el éxito de la operación.\n",
    "- El ciclo continúa avanzando al siguiente conjunto de páginas hasta que se hayan procesado todas las páginas disponibles para el año especificado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASO 2: CARGAR LOS DATOS A GOOGLE CLOUD BIGQUERY DESDE GOOGLE CLOUD STORAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código demuestra una solución robusta y escalable para la carga de datos en BigQuery desde archivos CSV almacenados en Google Cloud Storage (GCS). Es altamente adaptable y automatizable, lo que lo hace ideal para la recepción regular de datos actualizados, ya sea mensual, trimestral u otro período definido. La flexibilidad radica en su capacidad para crear dinámicamente tablas en BigQuery, detectar automáticamente el esquema de datos y cargar archivos CSV de manera eficiente desde GCS. Esto permite a las organizaciones mantener sus bases de datos actualizadas de manera fluida y sin problemas, adaptándose a las necesidades cambiantes de la empresa y garantizando la disponibilidad de información precisa para la toma de decisiones informadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las bibliotecas necesarias desde la biblioteca Google Cloud\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import BadRequest\n",
    "\n",
    "# Creamos un objeto cliente de BigQuery para interactuar con Google BigQuery\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Definimos una lista de años para la cual queremos realizar ciertas operaciones.\n",
    "# En este caso, solo estamos utilizando el año 2015 como ejemplo.\n",
    "lista_año = [2015,2016,2017,2018,2019,2020,2021,2022]\n",
    "\n",
    "# Iteramos a través de la lista de años\n",
    "for año in lista_año:\n",
    "    # Definimos el identificador de la tabla en BigQuery que deseamos crear y cargar\n",
    "    table_id = f\"{os.getenv('table_id')}.OCID_{año}\"\n",
    "\n",
    "    # Definimos un esquema vacío para la tabla, ya que vamos a autodetectar el esquema de los datos CSV.\n",
    "    schema = []\n",
    "\n",
    "    # Creamos un objeto de tabla de BigQuery con el identificador y el esquema especificados.\n",
    "    table = bigquery.Table(table_id, schema=schema)\n",
    "\n",
    "    # Creamos la tabla en BigQuery.\n",
    "    table = client.create_table(table)  # Hacemos una solicitud a la API.\n",
    "    print(\n",
    "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "    )\n",
    "\n",
    "    # Configuramos el trabajo de carga de datos en BigQuery.\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True, source_format=bigquery.SourceFormat.CSV\n",
    "    )\n",
    "\n",
    "    # Especificamos la URI del archivo CSV que deseamos cargar desde Google Cloud Storage (GCS).\n",
    "    uri = f\"gs://{os.getenv('bucket_name')}/datos/{año}/*.csv\"\n",
    "\n",
    "    try:\n",
    "        # Iniciamos un trabajo para cargar datos desde la URI especificada en la tabla de BigQuery.\n",
    "        load_job = client.load_table_from_uri(\n",
    "            uri, table_id, job_config=job_config\n",
    "        )  # Hacemos una solicitud a la API.\n",
    "\n",
    "        # Esperamos a que el trabajo de carga de datos se complete.\n",
    "        load_job.result()\n",
    "\n",
    "        # Obtenemos información sobre la tabla cargada.\n",
    "        destination_table = client.get_table(table_id)\n",
    "\n",
    "        # Imprimimos la cantidad de filas cargadas en la tabla.\n",
    "        print(\"Loaded {} rows.\".format(destination_table.num_rows))\n",
    "    except BadRequest as e:\n",
    "        # En caso de un error de solicitud (por ejemplo, archivo no encontrado), manejamos la excepción.\n",
    "        print(f\"Error loading data for year {año}: {e}\")\n",
    "        # Aquí podríamos tomar medidas para manejar el error, como registrarlo o saltar el archivo.\n",
    "        # También se proporciona un comentario que muestra cómo eliminar el archivo de GCS en caso de error.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
